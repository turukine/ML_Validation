{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML3 Validation Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "import optuna\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "np.random.seed(21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'R2': r2_score(y_true, y_pred)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ответы на вопросы\n",
    "\n",
    "1.1. Leave-one-out - это кросс-валидация где k=n (каждый образец используется как тест один раз).\n",
    "Сильные стороны: использует максимум данных для обучения, нет случайности, несмещенная оценка.\n",
    "Ограничения: вычислительно дорого (n моделей), высокая дисперсия, медленно для больших данных.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2. Grid Search - перебирает все комбинации параметров. Randomized Search - случайная выборка комбинаций. Bayesian Optimization - использует вероятностную модель (TPE) для предсказания лучших параметров на основе предыдущих оценок.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3. Классификация методов отбора признаков:\n",
    "- Unsupervised: не используют целевую переменную\n",
    "- Supervised: используют целевую переменную\n",
    "  - Filter: статистические меры (быстро, независимо от модели)\n",
    "  - Wrapper: производительность модели (медленно, зависит от модели)\n",
    "  - Embedded: встроены в обучение (Lasso, Ridge)\n",
    "\n",
    "Pearson - корреляция между признаком и целевой переменной (-1 до 1).\n",
    "Chi2 - проверка независимости для категориальных признаков.\n",
    "Lasso - добавляет штраф λ||w||₁, обнуляет неважные признаки.\n",
    "Permutation Importance - важность через перестановку значений признака.\n",
    "SHAP - объяснение предсказаний на основе Shapley values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Предобработка данных\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\slavc\\AppData\\Local\\Temp\\ipykernel_21212\\44242757.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_df['interest_level'] = train_df['interest_level'].replace({'low': 0, 'medium': 1, 'high': 2})\n"
     ]
    }
   ],
   "source": [
    "with open('data/train.json', 'r') as f:\n",
    "    train_df = pd.DataFrame(json.load(f))\n",
    "    \n",
    "with open('data/test.json', 'r') as f:\n",
    "    test_df = pd.DataFrame(json.load(f))\n",
    "\n",
    "train_df['interest_level'] = train_df['interest_level'].replace({'low': 0, 'medium': 1, 'high': 2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = ['Elevator', 'HardwoodFloors', 'CatsAllowed', 'DogsAllowed', 'Doorman', \n",
    "                'Dishwasher', 'NoFee', 'LaundryinBuilding', 'FitnessCenter', 'Pre-War', \n",
    "                'LaundryinUnit', 'RoofDeck', 'OutdoorSpace', 'DiningRoom', 'HighSpeedInternet', \n",
    "                'Balcony', 'SwimmingPool', 'LaundryInBuilding', 'NewConstruction', 'Terrace']\n",
    "\n",
    "for feat in feature_list:\n",
    "    train_df[feat] = train_df['features'].apply(lambda x: int(feat in x))\n",
    "    test_df[feat] = test_df['features'].apply(lambda x: int(feat in x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Методы разбиения данных\n",
    "\n",
    "3.1. Разбиение на 2 части случайно\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 39482, Test: 9870\n"
     ]
    }
   ],
   "source": [
    "def split_data_random_2(df, test_size=0.2, random_state=21):\n",
    "    np.random.seed(random_state)\n",
    "    n_samples = len(df)\n",
    "    n_test = int(n_samples * test_size)\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    test_indices = indices[:n_test]\n",
    "    train_indices = indices[n_test:]\n",
    "    return df.iloc[train_indices].copy(), df.iloc[test_indices].copy()\n",
    "\n",
    "train_split, test_split = split_data_random_2(train_df, test_size=0.2, random_state=21)\n",
    "print(f\"Train: {len(train_split)}, Test: {len(test_split)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2. Разбиение на 3 части случайно\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 29612, Val: 9870, Test: 9870\n"
     ]
    }
   ],
   "source": [
    "def split_data_random_3(df, validation_size=0.2, test_size=0.2, random_state=21):\n",
    "    np.random.seed(random_state)\n",
    "    n_samples = len(df)\n",
    "    n_test = int(n_samples * test_size)\n",
    "    n_val = int(n_samples * validation_size)\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    test_indices = indices[:n_test]\n",
    "    val_indices = indices[n_test:n_test+n_val]\n",
    "    train_indices = indices[n_test+n_val:]\n",
    "    return df.iloc[train_indices].copy(), df.iloc[val_indices].copy(), df.iloc[test_indices].copy()\n",
    "\n",
    "train_split, val_split, test_split = split_data_random_3(train_df, validation_size=0.2, test_size=0.2, random_state=21)\n",
    "print(f\"Train: {len(train_split)}, Val: {len(val_split)}, Test: {len(test_split)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3. Разбиение на 2 части по дате\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 24676, Test: 24676\n"
     ]
    }
   ],
   "source": [
    "def split_data_by_date_2(df, date_split, date_field='created'):\n",
    "    df[date_field] = pd.to_datetime(df[date_field])\n",
    "    date_split = pd.to_datetime(date_split)\n",
    "    return df[df[date_field] < date_split].copy(), df[df[date_field] >= date_split].copy()\n",
    "\n",
    "median_date = pd.to_datetime(train_df['created']).median()\n",
    "train_date, test_date = split_data_by_date_2(train_df, median_date)\n",
    "print(f\"Train: {len(train_date)}, Test: {len(test_date)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4. Разбиение на 3 части по дате\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 29611, Val: 9870, Test: 9871\n"
     ]
    }
   ],
   "source": [
    "def split_data_by_date_3(df, validation_date, test_date, date_field='created'):\n",
    "    df[date_field] = pd.to_datetime(df[date_field])\n",
    "    validation_date = pd.to_datetime(validation_date)\n",
    "    test_date = pd.to_datetime(test_date)\n",
    "    train_df = df[df[date_field] < validation_date].copy()\n",
    "    val_df = df[(df[date_field] >= validation_date) & (df[date_field] < test_date)].copy()\n",
    "    test_df = df[df[date_field] >= test_date].copy()\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "dates = pd.to_datetime(train_df['created'])\n",
    "val_date = dates.quantile(0.6)\n",
    "test_date = dates.quantile(0.8)\n",
    "train_date, val_date_split, test_date_split = split_data_by_date_3(train_df, val_date, test_date)\n",
    "print(f\"Train: {len(train_date)}, Val: {len(val_date_split)}, Test: {len(test_date_split)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5. Детерминированность - при одинаковом random_state результат всегда одинаковый\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Методы кросс-валидации\n",
    "\n",
    "4.1. K-Fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Фолдов: 5, первый фолд: train=39482, test=9870\n"
     ]
    }
   ],
   "source": [
    "def k_fold_cv(n_samples, k=5, random_state=21):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    fold_size = n_samples // k\n",
    "    folds = []\n",
    "    for i in range(k):\n",
    "        start = i * fold_size\n",
    "        end = (i + 1) * fold_size if i < k - 1 else n_samples\n",
    "        test_indices = indices[start:end]\n",
    "        train_indices = np.concatenate([indices[:start], indices[end:]])\n",
    "        folds.append((train_indices, test_indices))\n",
    "    return folds\n",
    "\n",
    "folds = k_fold_cv(len(train_df), k=5, random_state=21)\n",
    "print(f\"Фолдов: {len(folds)}, первый фолд: train={len(folds[0][0])}, test={len(folds[0][1])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2. Grouped K-Fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Фолдов: 5, первый фолд: train=39482, test=9870\n"
     ]
    }
   ],
   "source": [
    "def grouped_k_fold_cv(df, k=5, group_field='listing_id', random_state=21):\n",
    "    np.random.seed(random_state)\n",
    "    groups = df[group_field].unique()\n",
    "    np.random.shuffle(groups)\n",
    "    n_groups = len(groups)\n",
    "    fold_size = n_groups // k\n",
    "    folds = []\n",
    "    for i in range(k):\n",
    "        start = i * fold_size\n",
    "        end = (i + 1) * fold_size if i < k - 1 else n_groups\n",
    "        test_groups = groups[start:end]\n",
    "        test_mask = df[group_field].isin(test_groups)\n",
    "        test_indices = df[test_mask].index.values\n",
    "        train_indices = df[~test_mask].index.values\n",
    "        folds.append((train_indices, test_indices))\n",
    "    return folds\n",
    "\n",
    "folds = grouped_k_fold_cv(train_df, k=5, group_field='listing_id', random_state=21)\n",
    "print(f\"Фолдов: {len(folds)}, первый фолд: train={len(folds[0][0])}, test={len(folds[0][1])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3. Stratified K-Fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Фолдов: 5, первый фолд: train=39484, test=9868\n"
     ]
    }
   ],
   "source": [
    "def stratified_k_fold_cv(df, k=5, stratify_field='interest_level', random_state=21):\n",
    "    np.random.seed(random_state)\n",
    "    groups_by_class = {}\n",
    "    for idx, value in df[stratify_field].items():\n",
    "        if value not in groups_by_class:\n",
    "            groups_by_class[value] = []\n",
    "        groups_by_class[value].append(idx)\n",
    "    for class_val in groups_by_class:\n",
    "        np.random.shuffle(groups_by_class[class_val])\n",
    "    folds = [[] for _ in range(k)]\n",
    "    for class_val, indices in groups_by_class.items():\n",
    "        fold_size = len(indices) // k\n",
    "        for i in range(k):\n",
    "            start = i * fold_size\n",
    "            end = (i + 1) * fold_size if i < k - 1 else len(indices)\n",
    "            folds[i].extend(indices[start:end])\n",
    "    result = []\n",
    "    for i in range(k):\n",
    "        test_indices = np.array(folds[i])\n",
    "        train_indices = np.concatenate([np.array(folds[j]) for j in range(k) if j != i])\n",
    "        result.append((train_indices, test_indices))\n",
    "    return result\n",
    "\n",
    "folds = stratified_k_fold_cv(train_df, k=5, stratify_field='interest_level', random_state=21)\n",
    "print(f\"Фолдов: {len(folds)}, первый фолд: train={len(folds[0][0])}, test={len(folds[0][1])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4. Time Series Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Фолдов: 5, первый фолд: train=8225, test=8225\n"
     ]
    }
   ],
   "source": [
    "def time_series_split(df, k=5, date_field='created'):\n",
    "    df_sorted = df.sort_values(date_field).reset_index(drop=True)\n",
    "    n_samples = len(df_sorted)\n",
    "    fold_size = n_samples // (k + 1)\n",
    "    folds = []\n",
    "    for i in range(1, k + 1):\n",
    "        train_end = i * fold_size\n",
    "        test_end = (i + 1) * fold_size if i < k else n_samples\n",
    "        train_indices = df_sorted.index[:train_end].values\n",
    "        test_indices = df_sorted.index[train_end:test_end].values\n",
    "        folds.append((train_indices, test_indices))\n",
    "    return folds\n",
    "\n",
    "folds = time_series_split(train_df, k=5, date_field='created')\n",
    "print(f\"Фолдов: {len(folds)}, первый фолд: train={len(folds[0][0])}, test={len(folds[0][1])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Сравнение с sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Fold: наш=5, sklearn=5\n",
      "Grouped K-Fold: наш=5, sklearn=5\n",
      "Stratified K-Fold: наш=5, sklearn=5\n",
      "Time Series Split: наш=5, sklearn=5\n"
     ]
    }
   ],
   "source": [
    "our_kfold = k_fold_cv(len(train_df), k=5, random_state=21)\n",
    "sklearn_kfold = list(KFold(n_splits=5, shuffle=True, random_state=21).split(train_df))\n",
    "print(f\"K-Fold: наш={len(our_kfold)}, sklearn={len(sklearn_kfold)}\")\n",
    "\n",
    "our_grouped = grouped_k_fold_cv(train_df, k=5, group_field='listing_id', random_state=21)\n",
    "y = train_df['interest_level'].values\n",
    "groups = train_df['listing_id'].values\n",
    "sklearn_grouped = list(GroupKFold(n_splits=5).split(train_df, y, groups))\n",
    "print(f\"Grouped K-Fold: наш={len(our_grouped)}, sklearn={len(sklearn_grouped)}\")\n",
    "\n",
    "our_stratified = stratified_k_fold_cv(train_df, k=5, stratify_field='interest_level', random_state=21)\n",
    "sklearn_stratified = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=21).split(train_df, y))\n",
    "print(f\"Stratified K-Fold: наш={len(our_stratified)}, sklearn={len(sklearn_stratified)}\")\n",
    "\n",
    "our_timeseries = time_series_split(train_df, k=5)\n",
    "sklearn_timeseries = list(TimeSeriesSplit(n_splits=5).split(train_df))\n",
    "print(f\"Time Series Split: наш={len(our_timeseries)}, sklearn={len(sklearn_timeseries)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4. Выбрать лучшую схему валидации\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшая схема: Time Series Split (MSE=0.3918±0.0085)\n",
      "K-Fold: MSE=0.3920±0.0067\n",
      "Stratified K-Fold: MSE=0.3919±0.0003\n",
      "Time Series Split: MSE=0.3918±0.0085\n"
     ]
    }
   ],
   "source": [
    "feature_cols_cv = feature_list + ['bathrooms', 'bedrooms']\n",
    "X_cv = train_df[feature_cols_cv].fillna(0).values\n",
    "y_cv = train_df['interest_level'].values\n",
    "scaler_cv = StandardScaler()\n",
    "X_cv_scaled = scaler_cv.fit_transform(X_cv)\n",
    "\n",
    "idx_map = {idx: i for i, idx in enumerate(train_df.index)}\n",
    "\n",
    "cv_results = {}\n",
    "for name, folds in [('K-Fold', our_kfold), ('Stratified K-Fold', our_stratified), ('Time Series Split', our_timeseries)]:\n",
    "    scores = []\n",
    "    for train_idx, val_idx in folds:\n",
    "        if name == 'K-Fold':\n",
    "            train_idx = np.asarray(train_idx, dtype=int)\n",
    "            val_idx = np.asarray(val_idx, dtype=int)\n",
    "        elif name == 'Stratified K-Fold':\n",
    "            train_idx = np.array([idx_map[idx] for idx in train_idx], dtype=int)\n",
    "            val_idx = np.array([idx_map[idx] for idx in val_idx], dtype=int)\n",
    "        else:\n",
    "            train_idx = np.asarray(train_idx, dtype=int)\n",
    "            val_idx = np.asarray(val_idx, dtype=int)\n",
    "        model = Lasso(alpha=0.1, random_state=21)\n",
    "        model.fit(X_cv_scaled[train_idx], y_cv[train_idx])\n",
    "        scores.append(mean_squared_error(y_cv[val_idx], model.predict(X_cv_scaled[val_idx])))\n",
    "    cv_results[name] = {'mean': np.mean(scores), 'std': np.std(scores)}\n",
    "\n",
    "best_cv = min(cv_results.items(), key=lambda x: x[1]['mean'])\n",
    "print(f\"Лучшая схема: {best_cv[0]} (MSE={best_cv[1]['mean']:.4f}±{best_cv[1]['std']:.4f})\")\n",
    "for name, res in cv_results.items():\n",
    "    print(f\"{name}: MSE={res['mean']:.4f}±{res['std']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Отбор признаков\n",
    "\n",
    "6.1. Lasso модель с нормализацией (60/20/20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: train=0.3918, val=0.3900, test=0.3941\n"
     ]
    }
   ],
   "source": [
    "feature_cols = feature_list + ['bathrooms', 'bedrooms']\n",
    "train_split, val_split, test_split = split_data_random_3(train_df, validation_size=0.2, test_size=0.2, random_state=21)\n",
    "\n",
    "X_train = train_split[feature_cols].fillna(0).values\n",
    "X_val = val_split[feature_cols].fillna(0).values\n",
    "X_test = test_split[feature_cols].fillna(0).values\n",
    "y_train = train_split['interest_level'].values\n",
    "y_val = val_split['interest_level'].values\n",
    "y_test = test_split['interest_level'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lasso = Lasso(alpha=0.1, random_state=21)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "print(f\"MSE: train={mean_squared_error(y_train, lasso.predict(X_train_scaled)):.4f}, val={mean_squared_error(y_val, lasso.predict(X_val_scaled)):.4f}, test={mean_squared_error(y_test, lasso.predict(X_test_scaled)):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2. Топ 10 признаков по коэффициентам Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ 10: ['Elevator', 'HardwoodFloors', 'CatsAllowed', 'DogsAllowed', 'Doorman', 'Dishwasher', 'NoFee', 'LaundryinBuilding', 'FitnessCenter', 'Pre-War']\n",
      "MSE: val=0.3900, test=0.3941\n"
     ]
    }
   ],
   "source": [
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'coefficient': np.abs(lasso.coef_)\n",
    "}).sort_values('coefficient', ascending=False)\n",
    "\n",
    "top_10_lasso = feature_importance.head(10)['feature'].tolist()\n",
    "X_train_top10 = X_train_scaled[:, [feature_cols.index(f) for f in top_10_lasso]]\n",
    "X_val_top10 = X_val_scaled[:, [feature_cols.index(f) for f in top_10_lasso]]\n",
    "X_test_top10 = X_test_scaled[:, [feature_cols.index(f) for f in top_10_lasso]]\n",
    "\n",
    "lasso_top10 = Lasso(alpha=0.1, random_state=21)\n",
    "lasso_top10.fit(X_train_top10, y_train)\n",
    "print(f\"Топ 10: {top_10_lasso[:10]}\")\n",
    "print(f\"MSE: val={mean_squared_error(y_val, lasso_top10.predict(X_val_top10)):.4f}, test={mean_squared_error(y_test, lasso_top10.predict(X_test_top10)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3. Простой отбор по NaN-ratio и корреляции\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ 10: ['Doorman', 'bathrooms', 'Dishwasher', 'Pre-War', 'bedrooms', 'Terrace', 'Balcony', 'Elevator', 'HardwoodFloors', 'DogsAllowed']\n",
      "MSE: val=0.3900, test=0.3941\n"
     ]
    }
   ],
   "source": [
    "def simple_feature_selection(df, target_col, feature_cols, top_n=10):\n",
    "    scores = []\n",
    "    for feat in feature_cols:\n",
    "        nan_ratio = df[feat].isna().sum() / len(df)\n",
    "        nan_score = 1 - nan_ratio\n",
    "        try:\n",
    "            corr = abs(df[[feat, target_col]].corr().iloc[0, 1])\n",
    "            if np.isnan(corr):\n",
    "                corr = 0\n",
    "        except:\n",
    "            corr = 0\n",
    "        combined_score = 0.3 * nan_score + 0.7 * corr\n",
    "        scores.append((feat, combined_score))\n",
    "    scores_df = pd.DataFrame(scores, columns=['feature', 'score']).sort_values('score', ascending=False)\n",
    "    return scores_df.head(top_n)['feature'].tolist()\n",
    "\n",
    "top_10_simple = simple_feature_selection(train_df, 'interest_level', feature_cols, top_n=10)\n",
    "X_train_simple = X_train_scaled[:, [feature_cols.index(f) for f in top_10_simple]]\n",
    "X_val_simple = X_val_scaled[:, [feature_cols.index(f) for f in top_10_simple]]\n",
    "X_test_simple = X_test_scaled[:, [feature_cols.index(f) for f in top_10_simple]]\n",
    "\n",
    "lasso_simple = Lasso(alpha=0.1, random_state=21)\n",
    "lasso_simple.fit(X_train_simple, y_train)\n",
    "print(f\"Топ 10: {top_10_simple[:10]}\")\n",
    "print(f\"MSE: val={mean_squared_error(y_val, lasso_simple.predict(X_val_simple)):.4f}, test={mean_squared_error(y_test, lasso_simple.predict(X_test_simple)):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.4. Permutation importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ 10: ['Elevator', 'HardwoodFloors', 'CatsAllowed', 'DogsAllowed', 'Doorman', 'Dishwasher', 'NoFee', 'LaundryinBuilding', 'FitnessCenter', 'Pre-War']\n",
      "MSE: val=0.3900, test=0.3941\n"
     ]
    }
   ],
   "source": [
    "base_model = Lasso(alpha=0.1, random_state=21)\n",
    "base_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "perm_importance = permutation_importance(base_model, X_val_scaled, y_val, n_repeats=10, random_state=21, n_jobs=-1)\n",
    "perm_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance_mean': perm_importance.importances_mean\n",
    "}).sort_values('importance_mean', ascending=False)\n",
    "\n",
    "top_10_perm = perm_df.head(10)['feature'].tolist()\n",
    "X_train_perm = X_train_scaled[:, [feature_cols.index(f) for f in top_10_perm]]\n",
    "X_val_perm = X_val_scaled[:, [feature_cols.index(f) for f in top_10_perm]]\n",
    "X_test_perm = X_test_scaled[:, [feature_cols.index(f) for f in top_10_perm]]\n",
    "\n",
    "lasso_perm = Lasso(alpha=0.1, random_state=21)\n",
    "lasso_perm.fit(X_train_perm, y_train)\n",
    "print(f\"Топ 10: {top_10_perm[:10]}\")\n",
    "print(f\"MSE: val={mean_squared_error(y_val, lasso_perm.predict(X_val_perm)):.4f}, test={mean_squared_error(y_test, lasso_perm.predict(X_test_perm)):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.5. SHAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ 10: ['Elevator', 'HardwoodFloors', 'CatsAllowed', 'DogsAllowed', 'Doorman', 'Dishwasher', 'NoFee', 'LaundryinBuilding', 'FitnessCenter', 'Pre-War']\n",
      "MSE: val=0.3900, test=0.3941\n"
     ]
    }
   ],
   "source": [
    "explainer = shap.LinearExplainer(lasso, X_train_scaled)\n",
    "shap_values = explainer.shap_values(X_val_scaled[:100])\n",
    "shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "shap_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'shap_importance': shap_importance\n",
    "}).sort_values('shap_importance', ascending=False)\n",
    "\n",
    "top_10_shap = shap_df.head(10)['feature'].tolist()\n",
    "X_train_shap = X_train_scaled[:, [feature_cols.index(f) for f in top_10_shap]]\n",
    "X_val_shap = X_val_scaled[:, [feature_cols.index(f) for f in top_10_shap]]\n",
    "X_test_shap = X_test_scaled[:, [feature_cols.index(f) for f in top_10_shap]]\n",
    "\n",
    "lasso_shap = Lasso(alpha=0.1, random_state=21)\n",
    "lasso_shap.fit(X_train_shap, y_train)\n",
    "print(f\"Топ 10: {top_10_shap[:10]}\")\n",
    "print(f\"MSE: val={mean_squared_error(y_val, lasso_shap.predict(X_val_shap)):.4f}, test={mean_squared_error(y_test, lasso_shap.predict(X_test_shap)):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.6. Сравнить методы отбора признаков\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Метод  Val MSE  Test MSE\n",
      "Lasso coefficients 0.389953  0.394122\n",
      "  Simple selection 0.389953  0.394122\n",
      "       Permutation 0.389953  0.394122\n",
      "              SHAP 0.389953  0.394122\n",
      "\n",
      "Лучший метод: Lasso coefficients (Test MSE=0.3941)\n"
     ]
    }
   ],
   "source": [
    "comparison = pd.DataFrame({\n",
    "    'Метод': ['Lasso coefficients', 'Simple selection', 'Permutation', 'SHAP'],\n",
    "    'Val MSE': [\n",
    "        mean_squared_error(y_val, lasso_top10.predict(X_val_top10)),\n",
    "        mean_squared_error(y_val, lasso_simple.predict(X_val_simple)),\n",
    "        mean_squared_error(y_val, lasso_perm.predict(X_val_perm)),\n",
    "        mean_squared_error(y_val, lasso_shap.predict(X_val_shap))\n",
    "    ],\n",
    "    'Test MSE': [\n",
    "        mean_squared_error(y_test, lasso_top10.predict(X_test_top10)),\n",
    "        mean_squared_error(y_test, lasso_simple.predict(X_test_simple)),\n",
    "        mean_squared_error(y_test, lasso_perm.predict(X_test_perm)),\n",
    "        mean_squared_error(y_test, lasso_shap.predict(X_test_shap))\n",
    "    ]\n",
    "})\n",
    "print(comparison.to_string(index=False))\n",
    "best_method = comparison.loc[comparison['Test MSE'].idxmin(), 'Метод']\n",
    "print(f\"\\nЛучший метод: {best_method} (Test MSE={comparison['Test MSE'].min():.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Оптимизация гиперпараметров\n",
    "\n",
    "7.1. Grid Search и Random Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: {'alpha': 0.001, 'l1_ratio': 0.1}, MSE=0.3791\n",
      "Random Search: {'alpha': np.float64(0.001), 'l1_ratio': np.float64(0.1)}, MSE=0.3791\n"
     ]
    }
   ],
   "source": [
    "def grid_search_elasticnet(X_train, y_train, X_val, y_val, alpha_grid, l1_ratio_grid):\n",
    "    best_score = float('inf')\n",
    "    best_params = None\n",
    "    for alpha in alpha_grid:\n",
    "        for l1_ratio in l1_ratio_grid:\n",
    "            model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=21, max_iter=1000)\n",
    "            model.fit(X_train, y_train)\n",
    "            val_mse = mean_squared_error(y_val, model.predict(X_val))\n",
    "            if val_mse < best_score:\n",
    "                best_score = val_mse\n",
    "                best_params = {'alpha': alpha, 'l1_ratio': l1_ratio}\n",
    "    return best_params, best_score\n",
    "\n",
    "def random_search_elasticnet(X_train, y_train, X_val, y_val, alpha_range, l1_ratio_range, n_iter=25):\n",
    "    best_score = float('inf')\n",
    "    best_params = None\n",
    "    np.random.seed(21)\n",
    "    for _ in range(n_iter):\n",
    "        alpha = np.random.choice(alpha_range)\n",
    "        l1_ratio = np.random.choice(l1_ratio_range)\n",
    "        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=21, max_iter=1000)\n",
    "        model.fit(X_train, y_train)\n",
    "        val_mse = mean_squared_error(y_val, model.predict(X_val))\n",
    "        if val_mse < best_score:\n",
    "            best_score = val_mse\n",
    "            best_params = {'alpha': alpha, 'l1_ratio': l1_ratio}\n",
    "    return best_params, best_score\n",
    "\n",
    "alpha_grid = [0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "l1_ratio_grid = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "grid_params, grid_score = grid_search_elasticnet(X_train_scaled, y_train, X_val_scaled, y_val, alpha_grid, l1_ratio_grid)\n",
    "random_params, random_score = random_search_elasticnet(X_train_scaled, y_train, X_val_scaled, y_val, alpha_grid, l1_ratio_grid, n_iter=25)\n",
    "print(f\"Grid Search: {grid_params}, MSE={grid_score:.4f}\")\n",
    "print(f\"Random Search: {random_params}, MSE={random_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.2. Optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna: {'alpha': 0.0021097123443160645, 'l1_ratio': 0.14654038301385044}, MSE=0.3791\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    alpha = trial.suggest_float('alpha', 0.001, 10.0, log=True)\n",
    "    l1_ratio = trial.suggest_float('l1_ratio', 0.1, 0.9)\n",
    "    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=21, max_iter=1000)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    return mean_squared_error(y_val, model.predict(X_val_scaled))\n",
    "\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=21))\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=False)\n",
    "print(f\"Optuna: {study.best_params}, MSE={study.best_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.3. Optuna с кросс-валидацией\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna с CV: {'alpha': 0.0011333377686672955, 'l1_ratio': 0.21910664136704278}, CV MSE=0.3806\n"
     ]
    }
   ],
   "source": [
    "def objective_cv(trial):\n",
    "    alpha = trial.suggest_float('alpha', 0.001, 10.0, log=True)\n",
    "    l1_ratio = trial.suggest_float('l1_ratio', 0.1, 0.9)\n",
    "    cv_scores = []\n",
    "    folds = k_fold_cv(len(train_df), k=5, random_state=21)\n",
    "    for train_idx, val_idx in folds:\n",
    "        X_train_fold = train_df.iloc[train_idx][feature_cols].fillna(0).values\n",
    "        X_val_fold = train_df.iloc[val_idx][feature_cols].fillna(0).values\n",
    "        y_train_fold = train_df.iloc[train_idx]['interest_level'].values\n",
    "        y_val_fold = train_df.iloc[val_idx]['interest_level'].values\n",
    "        scaler_fold = StandardScaler()\n",
    "        X_train_fold_scaled = scaler_fold.fit_transform(X_train_fold)\n",
    "        X_val_fold_scaled = scaler_fold.transform(X_val_fold)\n",
    "        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=21, max_iter=1000)\n",
    "        model.fit(X_train_fold_scaled, y_train_fold)\n",
    "        cv_scores.append(mean_squared_error(y_val_fold, model.predict(X_val_fold_scaled)))\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "study_cv = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=21))\n",
    "study_cv.optimize(objective_cv, n_trials=30, show_progress_bar=False)\n",
    "print(f\"Optuna с CV: {study_cv.best_params}, CV MSE={study_cv.best_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.4. Сравнение методов оптимизации\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Метод  Итераций  Val RMSE  Test RMSE  Test MAE  Test R²\n",
      "  Grid Search        25  0.615688   0.618884  0.516090 0.028070\n",
      "Random Search        25  0.615688   0.618884  0.516090 0.028070\n",
      "       Optuna        50  0.615687   0.618884  0.516207 0.028071\n",
      "  Optuna с CV        30  0.616907   0.618885  0.516157 0.028069\n",
      "\n",
      "Лучший метод: Optuna (Test RMSE=0.6189)\n",
      "\n",
      "Вывод: Optuna нашла оптимальные параметры за 50 итераций против 25 у Grid Search.\n"
     ]
    }
   ],
   "source": [
    "grid_iter = len(alpha_grid) * len(l1_ratio_grid)\n",
    "random_iter = 25\n",
    "\n",
    "models = {\n",
    "    'Grid Search': (grid_params, grid_score),\n",
    "    'Random Search': (random_params, random_score),\n",
    "    'Optuna': (study.best_params, study.best_value),\n",
    "    'Optuna с CV': (study_cv.best_params, study_cv.best_value)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, (params, val_score) in models.items():\n",
    "    m = ElasticNet(**params, random_state=21, max_iter=1000)\n",
    "    m.fit(X_train_scaled, y_train)\n",
    "    test_metrics = calculate_all_metrics(y_test, m.predict(X_test_scaled))\n",
    "    results.append({\n",
    "        'Метод': name,\n",
    "        'Итераций': grid_iter if name == 'Grid Search' else (random_iter if name == 'Random Search' else (len(study.trials) if name == 'Optuna' else len(study_cv.trials))),\n",
    "        'Val RMSE': np.sqrt(val_score),\n",
    "        'Test RMSE': test_metrics['RMSE'],\n",
    "        'Test MAE': test_metrics['MAE'],\n",
    "        'Test R²': test_metrics['R2']\n",
    "    })\n",
    "\n",
    "opt_comparison = pd.DataFrame(results)\n",
    "print(opt_comparison.to_string(index=False))\n",
    "best_opt = opt_comparison.loc[opt_comparison['Test RMSE'].idxmin(), 'Метод']\n",
    "print(f\"\\nЛучший метод: {best_opt} (Test RMSE={opt_comparison['Test RMSE'].min():.4f})\")\n",
    "print(f\"\\nВывод: Optuna нашла оптимальные параметры за {len(study.trials)} итераций против {grid_iter} у Grid Search.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
